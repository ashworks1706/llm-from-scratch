# R1 Configuration for thinking and output phases
#
# LEARNING OBJECTIVES:
# - Separate hyperparameters for thinking and output token budgets
# - Configure reward model size relative to main model
# - Set KL divergence constraints for safe policy updates
# - Manage thinking token special token IDs
