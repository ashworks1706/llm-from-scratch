# comparing rl methods in practice
# when to use sft, dpo, ppo, or reward models

# topics to cover:
# - sft as baseline (no preference data needed)
# - dpo for simple preference optimization
# - ppo when you have reward model
# - computational cost comparisons
# - convergence speed and stability
# - which method for which use case
# - combining methods (sft -> dpo -> ppo)
# - modern trends (everyone uses dpo now)

# OBJECTIVE: understand tradeoffs between rl approaches
# make informed decisions on alignment strategy
