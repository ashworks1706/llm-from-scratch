# comparing lstm to transformer architecture
# understanding why transformers replaced lstms

# topics to cover:
# - sequential processing vs parallel processing
# - attention mechanism vs recurrence
# - long range dependencies
# - computational efficiency
# - why transformers won for nlp
# - when lstms are still useful
# - connection to the llama model you built
