# full rlhf with PPOTrainer
# most complex but most powerful alignment method

# topics to cover:
# - PPOTrainer vs DPOTrainer complexity
# - PPOConfig for hyperparameters
# - reward model integration
# - on policy generation (model generates responses)
# - advantage calculation from rewards
# - kl penalty preventing drift
# - why ppo is harder than dpo
# - batched generation for efficiency
# - value model for variance reduction

# OBJECTIVE: understand ppo pipeline even if not implementing
# see why industry moved to simpler dpo approach
