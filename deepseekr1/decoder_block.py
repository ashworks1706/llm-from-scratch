# Transformer decoder block combining attention and FFN
#
# LEARNING OBJECTIVES:
# - Stack attention and feedforward with residual connections
# - Use pre-normalization for training stability
# - Support thinking mask propagation through layers
# - Enable KV cache for efficient generation
