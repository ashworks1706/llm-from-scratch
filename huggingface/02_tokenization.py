# tokenizers convert text to numbers that models can process
# different models use different tokenization strategies

# topics to cover:
# - AutoTokenizer.from_pretrained()
# - encoding text to token ids
# - decoding token ids back to text
# - special tokens (bos, eos, pad, unk)
# - attention masks and padding
# - tokenizer.vocab_size
# - fast vs slow tokenizers
# - batch encoding with padding and truncation
# - understanding token ids vs input ids vs attention masks

# OBJECTIVE: understand how "hello world" becomes [123, 456, 789]
# practice encoding/decoding and handling special tokens
