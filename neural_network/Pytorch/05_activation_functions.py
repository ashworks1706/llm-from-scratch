# activation functions from scratch
# non linearity is what makes neural networks powerful

# topics to cover:
# - why we need activation functions (linearity problem)
# - relu: max(0, x)
# - sigmoid: 1 / (1 + exp(-x))
# - tanh: (exp(x) - exp(-x)) / (exp(x) + exp(-x))
# - leaky relu, gelu, swish
# - derivatives of each (for backprop understanding)
# - when to use which activation
