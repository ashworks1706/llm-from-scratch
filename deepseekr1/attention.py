# Multi-head attention with thinking token masking
#
# LEARNING OBJECTIVES:
# - Implement grouped query attention (KV heads < query heads)
# - Apply RoPE rotations to query and key vectors
# - Mask attention between thinking and output phases
# - Support efficient KV caching for generation
