# optimizers from scratch
# algorithms that update model parameters using gradients

# topics to cover:
# - gradient descent basics
# - sgd (stochastic gradient descent)
# - momentum
# - adam (adaptive moment estimation)
# - learning rate and its importance
# - weight decay (l2 regularization)
# - optimizer step and zero_grad
