# what are we doing?
# we have loss function, parameters (weights, biases) and our goal is to find theta that minimizes the loss 
#
# How can we do that?
# we can compute gradient of the layer, update parameters and repeat until convergence 
# âˆ‡L = âˆ‚L/âˆ‚Î¸ -> Î¸_new = Î¸_old - learning_rate * âˆ‡L -> repeat 
# 
# Why this works?
# Taylor seriesi approximation :
# L(Î¸ + Î”Î¸) â‰ˆ L(Î¸) + âˆ‡L Â· Î”Î¸
# To decrease loss, we want: L(Î¸ + Î”Î¸) < L(Î¸)
# This means: âˆ‡L Â· Î”Î¸ < 0
# Choose: Î”Î¸ = -Î· * âˆ‡L
# Then: âˆ‡L Â· Î”Î¸ = âˆ‡L Â· (-Î· * âˆ‡L) = -Î· * ||âˆ‡L||Â² < 0 âœ“

#  Moving opposite to gradient ALWAYS decreases loss (for small Î·)!

import torch
import torch.nn as nn 
import matplotlib.pyplot as plt


# 1. Vanilla Gradient Descent 
# Initialize: Î¸â‚€ = random values
# repeat until convergence by computing the loss L = f(theta_t) then use that loss in gradient g_t = grad(L), then update
# theta_{t+1} = theta_t - n * g_t where n is the learning rate (step size)
# determining the step size is really important, n too small, baby steps, too big, jump too far, n is supposed to be just right 
# 0 < n < 2/L where L is lipschitz constant (max curvature of loss)
# we zero out gradients after each iteration why ? gradients accumulate by default in pytorch
# Without zeroing:
# Step 1: grad = 20, update by -2
# Step 2: grad = 16 + 20 = 36 (!), update by -3.6
# Step 3: grad = 12.8 + 36 = 48.8 (!), ...
# â†’ Explodes!

# With zeroing:
# Step 1: grad = 20, update by -2, ZERO
# Step 2: grad = 16, update by -1.6, ZERO
# Step 3: grad = 12.8, ...
# â†’ Correct descent!

# code 

x = torch.tensor([10.0], requires_grad = true)
learning_rate = 0.2 
history_manual = []

for step in range(15):
    # forward pass 
    loss = x**2 
    # backward pass 
    loss.backward()

    history_manual.append((step, x.item(), loss.item(), x.grad.item())

    # manual update 
    with torch.no_grad():
        # move opposite to gradient, 
        # if gradient is positive (pointing uphill), subtract it 
        x -= learning_rate * x.grad 

    # zero out gradients 
    x.grad.zero_()

    if step%3==0 or step <3:
        step_info=history_manual[-1]
        print(f"Step {step_info[0]:2d}: x={step_info[1]:7.4f}, loss={step_info[2]:8.4f}, grad={step_info[3]:7.4f}")

print(f"\nâœ“ Converged to x = {x.item():.6f} (target: 0)")
print(f"  Loss decreased from 100 â†’ {loss.item():.6f}")





# SGD 
# the key difference between stochastic gradient and normal gradient is that in normal gradient descent 
# we do 1 update per epoch, but in SGD we do N/B updates per epoch where is B is number of batch 
# True gradient: âˆ‡L = (1/N) Î£áµ¢ âˆ‡Láµ¢
# (Average over ALL samples)

# SGD gradient: âˆ‡LÌ‚ = (1/B) Î£â±¼ âˆ‡L
# (Average over BATCH samples)

# âˆ‡LÌ‚ is NOISY estimate of âˆ‡L, but
# E[âˆ‡LÌ‚] = âˆ‡L (unbiased estimator!


# why SGD ? 
# the speed is way faster, since B << N and B/N times faster per update, can escape shallow local minima 
# the noise acts as regularization (what is regularization? to prevent overfitting in algorithms) 
# the trade off is that updates are a bit vague, need more total updates to converge but stil lfaster 

# code 
#
#


x = torch.tensor([10.0], requires_grad=True)
optimizer_sgd = torch.optim.SGD([x], lr=0.2)

history_sgd = []

for step in range(15):
    optimizer_sgd.zero_grad()
    loss = x ** 2 
    loss.backward()

    history_sgd.append((step, x.item(), loss.item(), x.grad.item()))

    optimizer_sgd.step() # this step updates the parameters, x = x - lr * x.grad

    if step%3==0 or step < 3:
        step_info = history_sgd[-1]
        print(f"Step {step_info[0]:2d}: x={step_info[1]:7.4f}, loss={step_info[2]:8.4f}, grad={step_info[3]:7.4f}")
    
print(f"\nâœ“ Same result as manual: x = {x.item():.6f}")



# SGD with momentum 
# wihtout momentum, gradient points perpendicular to ravine walls, zigzags back and forth, slow progress toward goal 
# with momentum, accumlates velocity in consistent direction, smooth path down ravine faster process 
#
#
# Velocity update (exponential moving average):
# v_t = Î² * v_{t-1} + (1-Î²) * g_t
     
# Parameter update:
# Î¸_t = Î¸_{t-1} - Î· * v_t

# Where:
# - Î² = momentum coefficient (typically 0.9)
# - g_t = current gradient
# - v_t = velocity (accumulated gradient)

# v_t = Î² * v_{t-1} + (1-Î²) * g_t
#    = (1-Î²) * g_t + Î² * [(1-Î²) * g_{t-1} + Î² * v_{t-2}]
#    = (1-Î²) * g_t + Î²(1-Î²) * g_{t-1} + Î²Â² * v_{t-2}
#    = (1-Î²) * [g_t + Î²*g_{t-1} + Î²Â²*g_{t-2} + Î²Â³*g_{t-3} + ...]

# Recent gradients weighted more, but ALL history contributes!

# With Î² = 0.9:
# g_t weighted by: (1-0.9) = 0.1
# g_{t-1} weighted by: 0.9*0.1 = 0.09
# g_{t-2} weighted by: 0.81*0.1 = 0.081
# ...

# Effective averaging over ~1/(1-Î²) = 10 steps

# Even though gradient is decreasing, velocity keeps it going!
# so with momentum, slower inititally but eventually converges faster 

# code 
#
#

# momentum adds inertia like a ball rolling downhill 
# Update rule: v_t = Î²*v_{t-1} + g_t
#              Î¸_t = Î¸_{t-1} - lr*v_t
# Where v = velocity (accumulated gradient), Î² = momentum coefficient (usually 0.9)

x = torch.tensor([10.0], requires_grad=True)
optimizer_momentum = torch.optim.SGD([x], lr=0.2, momentum=0.9)

history_momentum =[]

for step in range(15):
    optimizer_momentum.zero_grad()

    loss = x**2
    loss.backward()
    history_sgd.append((step, x.item(), loss.item(), x.grad.item()))
    optimizer_momentum.step() # internally maintains velocity state, first few steps: velocity builds up 

    if step%3==0 or step<3:
        step_info = history_momentum[-1]
        print(f"Step {step_info[0]:2d}: x={step_info[1]:7.4f}, loss={step_info[2]:8.4f}, grad={step_info[3]:7.4f}")
 
print(f"\nâœ“ Converged to x = {x.item():.6f}")



# ADAM - Adaptive Moment Estimation 
# the problems with SGD was that all parameters use same learning rate, 
# but some parameters need big steps, other need tiny steps 
# adam compute separate adaptive learning rate for each parameter, based on history of gradients for that parameter 
# 
# the hyperparameters it has are learning rate, exponential decay for first moment, exponential decay for second moment, numerical stability (expsilon)
# Initialize:
# mâ‚€ = 0 (first moment: mean of gradients) - The direction 
# first moment tells us the direction, which is the momentum, exponential moving average of gradients
# this smooths out the noise, points in consistent direction 
# vâ‚€ = 0 (second moment: variance of gradients) - The scale 
# this is the exponential moving average of squared gradients
# this measures how muhc gradietns vary, the greater this is, the larger or inconsistent gradeitns are and vice versa 
# t = 0 (timestep)

# At each step t:
# 1. Compute gradient: g_t = âˆ‡L

# 2. Update biased first moment:
#    m_t = Î²â‚ * m_{t-1} + (1-Î²â‚) * g_t
    
# 3. Update biased second moment:
#    v_t = Î²â‚‚ * v_{t-1} + (1-Î²â‚‚) * g_tÂ²
    
# 4. Bias correction (crucial for initial steps!):
#    mÌ‚_t = m_t / (1 - Î²â‚áµ— )
#    vÌ‚_t = v_t / (1 - Î²â‚‚áµ— )
# this is needed since early estimats are biased toward 0
    
# 5. Update parameters:
#    Î¸_t = Î¸_{t-1} - Î· * mÌ‚_t / (âˆšvÌ‚_t +

# so final update becomes 
# Î¸_t = Î¸_{t-1} - Î· * mÌ‚_t / (âˆšvÌ‚_t + 
#            â†‘    â†‘      â†‘
#            |    |      |
#            base   direction  adaptive
#            LR     (momentum) scaling

# Per-parameter adaptive learning rate:
# Î·_effective = Î· / (âˆšvÌ‚_t + Îµ

# If parameter has:
# - Large gradients â†’ large vÌ‚ â†’ small effective Î· (take smaller steps
# - Small gradients â†’ small vÌ‚ â†’ large effective Î· (take bigger steps
# - Inconsistent gradients â†’ large vÌ‚ â†’ small effective Î· (be cautious
# - Consistent gradients â†’ small vÌ‚ â†’ large effective Î· (be confident


# code 
#
#

x = torch.tensor([10.0], requires_grad=True)
optimizer_adam = torch.optim.ADAM([x], lr=0.5)

history_adm = []

for step in range(15):
    optimizer_adam.zero_grad()

    loss = x**2
    loss.backward()

    state = optimizer_adam.state[x]

    optimizer_adam.step()

    history_adm.append((step, x.item(), loss.item(), x.grad.item()))

    if step%3==0 or step<3:
        step_info = history_adm[-1]
        print(f"Step {step_info[0]:2d}: x={step_info[1]:7.4f}, loss={step_info[2]:8.4f}, grad={step_info[3]:7.4f}")


print(f"\nâœ“ Converged to x = {x.item():.6f}")

# ADAMW - What normal LLM uses 
# the weight decay issue with adam optimizers is that L2 regularization is applies weight decay to gradeint before adaptive scaling 
# in turn weight decay gets scaled differently per parameter, which is not true L2 reg 
# this is why we use ADAMW for computing updates with Adam (no weight decay in gradient)
# apply weight decay directly to weights:
# AdamW update:
#       Î¸_t = Î¸_{t-1} - Î· * mÌ‚_t / (âˆšvÌ‚_t + Îµ) - Î·*Î»*Î¸_{t-
#             â†‘___________________________â†‘   â†‘__________â†‘
#                   Adam update            weight decay
     
#     Separating these makes weight decay not affected by adaptive scaling! Î¸_t = Î¸_{t-1} - Î·*update - Î·*Î»*Î¸_{t-1}
          

# code 
#
#
x = torch.tensor([10.0], requires_grad=True)
optimizer_adamw = torch.optim.AdamW([x], lr=0.5, weight_decay=0.01)

history_adamw = []

for step in range(15):
    optimizer_adamw.zero_grad()
    loss = x ** 2
    loss.backward()
    
    history_adamw.append((step, x.item(), loss.item(), x.grad.item() if step < 14 else 0))
    
    # Weight decay pulls x toward 0 even without gradient!
    # This is regularization - prevents weights from growing too large
    optimizer_adamw.step()
    
    if step % 3 == 0 or step < 3:
        print(f"Step {step:2d}: x={x.item():7.4f}, loss={loss.item():8.4f}")

print(f"\nâœ“ Converged to x = {x.item():.6f}")
#   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#   â”‚ Optimizer    â”‚ Memory    â”‚ Speed  â”‚ Hyperparams to Tune â”‚ Typical Use                               â”‚
#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
#   â”‚ SGD          â”‚ Î¸ only    â”‚ Fast   â”‚ lr, momentum        â”‚ Vision (ResNet), well-understood problems â”‚
#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
#   â”‚ SGD+Momentum â”‚ Î¸ + v     â”‚ Fast   â”‚ lr, Î²               â”‚ Vision, smoother landscapes               â”‚
#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
#   â”‚ Adam         â”‚ Î¸ + m + v â”‚ Medium â”‚ lr (usually 0.001)  â”‚ NLP, most DL problems, default choice     â”‚
#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
#   â”‚ AdamW        â”‚ Î¸ + m + v â”‚ Medium â”‚ lr, weight_decay    â”‚ Transformers, LLMs (your Llama!)          â”‚
#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜







print("COMPARISON OF ALL OPTIMIZERS")
print("="*60)

# Print final values
print("\nðŸ“Š Final x values (target: 0):")
print(f"   Manual GD:  {history_manual[-1][1]:.6f}")
print(f"   SGD:        {history_sgd[-1][1]:.6f}")
print(f"   Momentum:   {history_momentum[-1][1]:.6f}")
print(f"   Adam:       {history_adam[-1][1]:.6f}")
print(f"   AdamW:      {history_adamw[-1][1]:.6f}")

# Convergence speed comparison
print("\nâš¡ Convergence speed (steps to reach x < 0.1):")
for name, hist in [("Manual GD", history_manual), ("SGD", history_sgd), 
                    ("Momentum", history_momentum), ("Adam", history_adam), 
                    ("AdamW", history_adamw)]:
    steps = next((i for i, (step, x, _, _) in enumerate(hist) if abs(x) < 0.1), len(hist))
    print(f"   {name:12s}: {steps} steps")





























