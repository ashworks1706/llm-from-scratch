# automatic differentiation - the magic behind backpropagation
# pytorch automatically computes gradients using computational graphs

# topics to cover:
# - what is autograd and why it matters
# - requires_grad flag
# - backward() method
# - computational graph construction
# - gradient accumulation
# - detach() and no_grad()
# - manual gradient computation vs autograd
