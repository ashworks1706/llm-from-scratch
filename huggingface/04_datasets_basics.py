# loading and processing datasets efficiently
# datasets library handles data too large for memory

# topics to cover:
# - load_dataset() from hub
# - dataset streaming for large datasets
# - dataset structure (features, splits, num_rows)
# - indexing and slicing datasets
# - dataset.map() for applying transformations
# - batched mapping for efficiency
# - dataset.filter() for selecting subsets
# - dataset.train_test_split()
# - caching and when to use load_from_disk()

# OBJECTIVE: load a dataset, tokenize it, and prepare it for training
# understand lazy evaluation vs eager execution
