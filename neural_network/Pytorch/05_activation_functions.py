# activation functions from scratch
# non linearity is what makes neural networks powerful


# so the problem with neural networks was that whtout activations, it was becoming increasingly hard to code it out like a big chain equations
# so we added activation functions to add one linear layer operations as a stack 

# activations break linearity so netwroks can learn complex pattersn 



# ReLU -> Rectified Linear Unit 
# ReLU(x) = max(0, x) = {
#         x   if x > 0
#         0   if x â‰¤ 0
#     }
